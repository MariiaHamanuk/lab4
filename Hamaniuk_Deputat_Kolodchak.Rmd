---
title: "R Notebook"
output: html_notebook
editor_options: 
  markdown: 
    wrap: 72
---

**Work breakdown structure estimating the efforts of each team
member:**\
Task 1-2 did Anton Deputat and he did 33.33% of work.\
Task 3 + general conclusions did Bohdan Kolodchak and he did 33.33% of
work.\
Task 4 did Mariia Hamaniuk and she did 33.33% of work.

**Problem 1.** $H_0$ : $µ_1$ = $µ_2$ vs. $H_1$ : $µ_1$ ̸= $µ_2$;

$\sigma^2_1$ = $\sigma^2_2$ =1

Before starting lets create some data:

```{r}
n <- 32 #id number

# Function to calculate a_k = {k * log(k / (2*n) + pi)}
ak_func <- function(k, n) {
  val <- k * log(k / (2 * n) + pi)
  return(val - floor(val))
}

k_values <- 1:150
a_data <- ak_func(k_values, n)

# Generate samples X and Y using qnorm
# X: k = 1 to 100
X <- qnorm(a_data[1:100])
# Y: k = 101 to 150 (l = 1 to 50, k = l + 100)
Y <- qnorm(a_data[101:150])

# Sample sizes and sample means/variances
n1 <- length(X)
n2 <- length(Y)
mean1 <- mean(X)
mean2 <- mean(Y)
var1 <- var(X) 
var2 <- var(Y) 

cat(paste("n1 (Length of X):", n1, "\n"))
cat(paste("n2 (Length of Y):", n2, "\n"))
cat(paste("Sample Mean (X):", mean1, "\n"))
cat(paste("Sample Mean (Y):", mean2, "\n"))
cat(paste("Sample Variance (X):", var1, "\n"))
cat(paste("Sample Variance (Y):", var2, "\n"))
```

**Problem's Description:**

Hypotheses: $H_0: \mu_1 = \mu_2$$H_1: \mu_1 \neq \mu_2$

Assumptions: $\sigma_1^2 = \sigma_2^2 = 1$ (known)

Significance Level: $\alpha = 0.05$

**1. Standard Test Used and Why**

The test used is the **Two-Sample Z-Test**.

-   **Rationale:** We are comparing the means of two independent,
    normally distributed populations, and crucially, the **population
    variances (**$\sigma^2_1$ and $\sigma^2_2$) are known (both equal to
    1). The Z-test is appropriate when population variances are known,
    regardless of sample size.

-   **Test Statistic Formula:**
    $Z = \frac{\bar{X} - \bar{Y}}{\sqrt{\frac{\sigma^2_1}{n_1} + \frac{\sigma^2_2}{n_2}}} \sim N(0, 1)$

**2. General Form of the Rejection Region**

The test is **two-tailed** ($\text{H}_1: \mu_1 \neq \mu_2$).

-   For $\alpha=0.05$, we find the critical values $z_{\alpha/2}$ and
    $z_{1-\alpha/2}$ from the standard normal distribution.

-   $z_{\text{crit}} = \Phi^{-1}(0.975) \approx 1.960$

-   **Rejection Region:** Reject $\text{H}_0$ if $|Z| \geq 1.960$.

**R Code:**

```{r}
sigma2 <- 1 
n1 <- length(X)
n2 <- length(Y)
p_value_z <- 2 * pnorm(z_stat) # pnorm is P(Z <= z_stat)

# Calculate Z test statistic
z_stat <- (mean(X) - mean(Y)) / sqrt(sigma2/n1 + sigma2/n2)
cat(paste("Z Test Statistic:", z_stat, "\n"))
cat(paste("P-value:", p_value_z, "\n"))
# Critical value for alpha = 0.05, two-tailed
z_crit <- qnorm(0.975)
cat(paste("Critical Value (z_0.975):", z_crit, "\n"))

# Check rejection
reject_H0_z <- abs(z_stat) >= z_crit
cat(paste("Reject H0 at 0.05 level?", reject_H0_z, "\n"))


```

**The statistics obtained (like sample mean or anything else you use to
complete the task):**

**Conclusions:\
**Test Statistic calculations:

$$
Z = \frac{-0.158 - (-0.369)}{\sqrt{\frac{1}{100} + \frac{1}{50}}}
= \frac{0.211}{\sqrt{0.03}}
\approx 1.218
$$

-   **Calculated Z-statistic:** \$\$Z \\approx 1.353\$

-   **Comparison:** $|Z| = 1.353$ is **less than** the critical value
    $1.960$.

-   **Decision:** We **do not reject** $\text{H}_0$ at the $0.05$
    significance level.

**P-value:** The probability of observing a Z-statistic as extreme as
$1.353$ in a two-tailed test is $2 \times P(Z \geq 1.353)$.

Since the P-value of $0.176$ is **greater than** the significance level
$\alpha=0.05$, we do not reject the null hypothesis. There is not enough
evidence to conclude that the population means $\mu_1$ and $\mu_2$ are
different.

**Problem 2.** $H_0$ : $\sigma^2_1$ = $\sigma^2_2$ vs. $H_1$ :
$\sigma^2_1$ \> $\sigma^2_2$ ; $µ_1$ and $µ_2$ are unknown. (Hint: this
is the f-test; read the details in Ross, p. 321–323)

**Problem's Description:**

Hypotheses: $H_0: \sigma_1^2 = \sigma_2^2$ vs
$H_1: \sigma_1^2 > \sigma_2^2$

Assumptions: $\mu_1$ and $\mu_2$ are unknown.

Significance Level: $\alpha = 0.05$

**1. Standard Test Used and Why**

The test used is the **F-Test for Equality of Two Variances**.

-   **Rationale:** We are comparing the ratio of two population
    variances ($\sigma^2_1$ and $\sigma^2_2$) from two independent,
    normally distributed samples.

-   **Test Statistic Formula:**
    $F = \frac{S^2_1}{S^2_2} \sim F(n_1-1, n_2-1)$

    -   Degrees of freedom: $df_1 = 100 - 1 = 99$ (Numerator) and
        $df_2 = 50 - 1 = 49$ (Denominator).

**2. General Form of the Rejection Region**

The test is **right-tailed** ($\text{H}_1: \sigma^2_1 > \sigma^2_2$).

-   For $\alpha=0.05$, we find the critical value $f_{1-\alpha}$ from
    the F-distribution $F(99, 49)$.

-   $f_{\text{crit}} = F_{0.95}(99, 49) \approx 1.583$

-   **Rejection Region:** Reject $\text{H}_0$ if $F \geq 1.583$.

**R code:**

```{r}

# Perform the F-test
f_test_result <- var.test(X, Y, alternative = "greater")

# Extract results
f_stat <- f_test_result$statistic
p_value_f <- f_test_result$p.value
df1 <- f_test_result$parameter[1]
df2 <- f_test_result$parameter[2]

cat(paste("F Test Statistic:", f_stat, "\n"))
cat(paste("Degrees of Freedom:", df1, "and", df2, "\n"))
cat(paste("P-value:", p_value_f, "\n"))

# Critical value (f_0.95)
f_crit <- qf(0.95, df1 = df1, df2 = df2)
cat(paste("Critical Value (f_0.95):", f_crit, "\n"))

# Check rejection
reject_H0_f <- f_stat >= f_crit
cat(paste("Reject H0 at 0.05 level?", reject_H0_f, "\n"))
```

**the statistics obtained (like sample mean or anything else you use to
complete the task):**

**Conclusions:**

-   Test Statistic Calculation:

    $$
    F = \frac{S_1^2}{S_2^2} = \frac{1.258}{1.000} \approx 1.258
    $$

-   **Calculated F-statistic:** $F \approx 1.258$

-   **Comparison:** $F = 1.258$ is **less than** the critical value
    $1.583$.

-   **Decision:** We **do not reject** $\text{H}_0$ at the $0.05$
    significance level.

-   **P-value**: The probability of observing an F-statistic as large as
    \$1.258\$ in the \$F(99, 49)\$ distribution.

    Since the P-value of $0.165$ is **greater than** the significance
    level $\alpha=0.05$, we do not reject the null hypothesis. There is
    no statistically significant evidence to conclude that the
    population variance of X ($\sigma^2_1$) is greater than the
    population variance of Y ($\sigma^2_2$).

**Problem 3.** Using Kolmogorov–Smirnov test in R, check if (a)
${ x_k }_{k=1}^{100}$ are normally distributed (with parameters
calculated from the sample); (b)${|x_k|}_{k=1}^{100}$ are exponentially
distributed with λ = 1; (c) ${ x_k }_{k=1}^{100}$ and
${ y_l }_{l=1}^{50}$ have the same distribution

**Connections with Previous Problems:** In this analysis, we utilized
the sample means and variances calculated in Problems 1 and 2 to define
the parameters for the theoretical distributions. These estimates
allowed us to construct the reference distributions for the one-sample
Kolmogorov–Smirnov tests.

#### Quick overview of Kolmogorov-Smirnov test

The Kolmogorov--Smirnov (KS) test is a nonparametric statistical test
that compares a sample with a reference probability distribution
(one-sample KS test) or compares two samples (two-sample KS test). The
main idea is to measure the maximum difference between the empirical
cumulative distribution function (ECDF) of the sample(s) and the
theoretical or other empirical CDF. The test statistic is defined as

$$
D = \sup_x |F_n(x) - F_0(x)|
$$

for the one-sample test, or

$$
D = \sup_x |F_X(x) - F_Y(x)|
$$

for the two-sample KS test, where $F_n$ is the empirical CDF and $F_0$
or $F_Y$ is the theoretical or second sample CDF. The KS test is widely
used because it is sensitive to differences in location, scale, and
shape of the distributions without making parametric assumptions. It is
particularly useful for checking goodness-of-fit and comparing
distributions.

##### a) Let\`s find out if our data is coresponding to our teoreticaly

```         
calculated parametrs for normal distribution.
```

Let's generate the theoretical data from normal distribution with
parametrs which we found.

```{r}
x_sorted <- sort(X)
Fn <- ecdf(X)(x_sorted)
F0 <- pnorm(x_sorted, mean1, sqrt(var1))

max_dif <- max(abs(Fn - F0))
cat("Max difference calculated by hand", max_dif, "\n")

ks.test(X, "pnorm", mean1, sqrt(var1))

plot(x_sorted, Fn, type = "s", col = "blue", lwd = 2,
     xlab = "X", ylab = "CDF",
     main = "Empirical CDF vs Theoretical Normal CDF")
lines(x_sorted, F0, col = "red", lwd = 2)
legend("topleft", legend = c("Empirical CDF", "Theoretical CDF"),
       col = c("blue", "red"), lwd = 2)

qqnorm(X, main = "QQ-Plot of X vs Normal Distribution")
qqline(X, col = "red", lwd = 2)

```

**Comment:** In this part, we compare the empirical distribution of the
sample X with the theoretical normal distribution having the mean and
variance estimated from the data. We compute the maximum vertical
difference between the empirical CDF and the theoretical CDF, and then
apply the one-sample Kolmogorov–Smirnov test. The KS test is
particularly powerful because it is nonparametric and sensitive to any
type of deviation (shifts, scaling, skewness, or tail behavior), unlike
tests that target specific parameters. The resulting KS statistic (D =
0.0516) is small, and the high p-value (0.9525) indicates that the
sample is fully consistent with the assumed normal model.

##### b) Let\`s check if our X are similar to exponential distribution with $\lambda = 1$

```{r}
x_sorted <- sort(abs(X))
Fn <- ecdf(abs(X))(x_sorted)
F0 <- pexp(x_sorted, rate = 1)
max_dif <- max(abs(Fn - F0))
cat("Max difference calculated by hand", max_dif, "\n")

ks.test(abs(X), "pexp", 1)

plot(x_sorted, Fn, type = "s", col = "blue", lwd = 2,
     xlab = "X", ylab = "CDF",
     main = "Absolute Empirical CDF vs Theoretical Exponential CDF")
lines(x_sorted, F0, col = "red", lwd = 2)
legend("topleft", legend = c("Absolute Empirical CDF", "Theoretical CDF"),
       col = c("blue", "red"), lwd = 2)

qqnorm(x_sorted, main = "QQ-Plot of Absolute X vs Exponential Distribution")
qqline(x_sorted, col = "red", lwd = 2)

```

**Comment:** The maximum vertical difference between the empirical CDF
of $∣X∣$ and the theoretical exponential CDF is moderate ($D≈0.136$).
The p-value (0.05064) is very close to the 0.05 significance level,
indicating borderline evidence against the null hypothesis. We are at
the threshold, so the result is marginally consistent with an
exponential distribution with $λ=1$. Therefore, the sample of absolute
values of X may be considered roughly consistent with the exponential
model. This suggests that taking absolute values reduces the deviation
compared to using the original X values.

##### c) Let\`s find out if X has the same distribution as Y based on your generated data

```{r}
T <- sort(c(X, Y))
F_X <- sapply(T, function(t) mean(X <= t))
F_Y <- sapply(T, function(t) mean(Y <= t))
max_dif <- max(abs(F_X - F_Y))
cat("Max difference calculated by hand", max_dif, "\n")

ks.test(X, Y)

plot(T, F_X, type = "s", col = "blue", lwd = 2,
     xlab = "Value", ylab = "ECDF",
     main = "Empirical CDFs of X and Y")
lines(T, F_Y, col = "red", lwd = 2)
legend("topleft", legend = c("ECDF X", "ECDF Y"),
       col = c("blue", "red"), lwd = 2)

qqplot(X, Y, main = "QQ-Plot: X vs Y",
       xlab = "X Quantiles", ylab = "Y Quantiles")
abline(0, 1, col = "red", lwd = 2)

```

**Comment:** The maximum vertical difference between the empirical CDFs
of X and Y is moderate ($D≈0.23$). The p-value (0.0555) is slightly
above the 0.05 significance level, indicating no strong evidence against
the null hypothesis. We do not reject $H0$ at the 0.05 level. Therefore,
the samples X and Y can be considered consistent with having the same
distribution. This suggests that there is no statistically significant
difference between the two empirical distributions.

**Conclusions:** Based on the KS tests, the sample $X$ is consistent
with a normal distribution using its estimated parameters, the absolute
values of $X$ are roughly consistent with an exponential distribution
with $\lambda = 1$, and the samples $X$ and $Y$ do not show a
statistically significant difference in their distributions. Therefore,
the empirical evidence supports that the data generally aligns with the
assumed theoretical models.

The ECDF plots and QQ-plots visually confirm these findings: the
empirical distributions align closely with the theoretical curves, the
quantiles of X and Y match well along the diagonal. Therefore, the
empirical evidence supports that the data generally aligns with the
assumed theoretical models, and the graphical representations reinforce
the results of the KS tests.

**Problem 4.** In this task you’ll practice fitting the regression line
to some real-life features and analyzing the results. The file data.csv
contains data on students, specifically their study time and
corresponding marks. Your tasks are as follows:

(a) Create a scatter plot of Marks vs. Study Time and provide brief
    comments;

```{r}
id <- 19
data <- read.csv("C:/Users/Mariia Hamaniuk/Desktop/r/lab4HamaniukDeputatKolodchak/data.csv")
plot(data$time_study, data$Marks,
     xlab = "Study Time",
     ylab = "Marks",
     pch = 18, col = "black")
#min(data$time_study)
#max(data$time_study)
#min(data$Marks)
#max(data$Marks)
```

First of all the relationship is not 100% linear it has small curve, so
simple linear model can a bit misfit our data. The relationship is
positive which means: the longer you study the better results you get
(this is not surprise). There is no visible and strong (extreme)
outliers here. Range of data goes from 1 to 8 (time of study), 5-55
grades. If we want to predict something bigger or smaller than what we
have in data, we shouldn't (we simply don't know anything of that data
maybe it would be linear maybe not, maybe no one can score more than
55).

(b) Fit a linear regression model using marks as the dependent variable
    and study time as the independent variable.

```{r}
model <- lm(Marks ~ time_study, data = data)
```

visualization with fit

```{r}
plot(data$time_study, data$Marks,
     main = "Marks vs. Study Time",
     xlab = "Study Time (Hours)",
     ylab = "Marks",
     pch = 18, col = "black")
abline(model, col = "red", lwd = 2)
```

```{r}
# we can use formulas that are already in r 
a <- coef(model)["(Intercept)"]
b <- coef(model)["time_study"]
cat("regression equation:\n mark = ", a, " + ", b, " * x\n (where x is the time person studied)")
```

```{r}
#alterantively we can calculate manually as follows
x <- data$time_study
y <- data$Marks

b_hat <- sum((x - mean(x)) * (y - mean(y))) / sum((x - mean(x))^2)

a_hat <- mean(y) - (b_hat * mean(x))

cat("a", a_hat, "\n")
cat("b", b_hat, "\n")
#and we can clearly see that they are the same

```

Explain shortly the process of deriving the regression equation:

we have to estimate this equation:

$$\text{Mark} = a + \beta \times \text{time_study} + \epsilon$$

residual = $mark_i - \hat{mark_i}$

$\text{SumSquaredResiduals} = \sum_{i=1}^{n} (\text{mark} - \hat{\text{mark}})^2 = \sum_{i=1}^{n} (\text{mark} - (\hat{a} + \hat{\beta} \times \text{time_study}_i))^2$

We need to minimize SSR by taking derivatives on a and b (derivatives
after simplifing and plugining in estimator of beta):

$\hat{a} = \overline{\text{mark}} - \hat{\beta} * \overline{\text{time_study}}$

$\hat{\beta} = \frac{\sum_{i=1}^{n} (\text{time_study}_i - \overline{\text{time_study}})*(\text{mark}_i - \overline{\text{mark}})}{\sum_{i=1}^{n} (\text{time_study}_i - \overline{\text{time_study}})^2}$

so we used OLS here.

(c) Evaluate the goodness-of-fit for the fitted line;

    ```{r}
    summary(model)
    ```

    We evaluate goodness of fit with coef. of determination or $R^2$ and
    it equals to 0.88 which means that 88% of variability in marks is
    explained by time of study. So we can say that study time is
    actually pretty strong predictor. Let\`s consider other metrics such
    as location: median is not really but still close to 0 which means
    that it is not consistent in similar making errors (always
    predicting more or predicting less). From this parameters we can say
    that the fit is good but could be better (no optimal because our
    data's relationship is not strictly linear)

(d) Suggest a way to test whether the study time is significant in
    predicting marks. Find the corresponding test statistics, specify
    the distribution. Find p-value of the test and make a conclusion;

    $H_0$: $\beta_1 = 0$ - which means that there is no linear
    relationship of study time and marks. b = 0

    $H_1$: $\beta_1 \neq 0$ - contrary there is linear relationship. b
    does not = 0;

    We need to estimate how many units are between estimate and
    hypothetic value. We have only one sample so we can use Z-statistic
    or T-statistic but we don't really have a variance so it is better
    to use T-statistic. for t0, a = 0;

    ${t_0} = \frac{\hat{\beta} - 0}{\text{SE}(\hat{\beta})}$

    se = $\sqrt(mse/Sxx)$

    mse = $RSE^2$ - this thing we take from summary

    ```{r}
    b - 0
    mse <- 4.822^2
    sxx <- sum((x - mean(x))^2)
    se <- sqrt(mse/sxx)
    se
    b / se
    ```

    $t = \frac{5.68875}{0.2042338} = \mathbf{27.85411}$

    Under $H_0$, this statistic follows a Student distr. with n - (num
    of predictors) - 1degrees of freedom so 98 because one predictor is
    0 and error in our equation is obviously not a predictor.

    p-value is almost zero so we can reject $H_0$

```{r}
summary(model)
```

e\. If Alice studies for approximately 8 hours, what grade can we
predict for her?

let's do it manually and then using the code and compare

```         
 1.223858 + 5.68875 * 8 = 46.7343
```

```{r}
study_time_alice <- 8
print(a + b * study_time_alice)

```

One interesting thing that I noticed about this task and linear
regressions at all (it is a really obvious but when you have real data
and we had marks which are in range 1-100 ) and we have limited data
(not much data and not in a full range) and we try to predict for values
in my case larger than 18 than it gives us values larger than 100 how to
deal with that? everything that is larger cut to 100 and same for less
than 0? And in general it is a bit dangerous to put value that are not
in out data set.

f\. Suggest up to three ideas to potentially improve prediction
accuracy:

1.  first thing that comes to mind is to add regions here or any other
    factor that can influence output (private / public school, previous
    results...). Because it depends whether you study in small city or
    big, developing or developed country.
2.  Make more test but it is expensive.
3.  We can use some other type of regression not linear but something
    curved. (like polynomial, parabola)

**the statistics obtained (like sample mean or anything else you use to
complete the task)**:

```         
median: -0.384
min: -7.866
max: 10.628 
Residual standard error: 4.822 on 98 degrees of freedom 
Multiple R-squared:  0.8878,    
Adjusted R-squared:  0.8867  
p-value: < 2.2e-16
df: 98
F-statistic: 775.8
F-statistic DF: 1 and 98
```

**Conclusions:**

Since the P-value for time of study is really close to 0 we reject null
hypothesis$H_0: \beta = 0$, which means study time is a significant
predictor of marks. The $R^2$ of $0.8878$ indicates a strong overall
fit. The model explains almost 89% of the variation in student marks.
Despite that just visualy one can tell that we can clearly do better
than just a line when estimating (because our data creates a bit curved
line). This nonoptimality is not only visually derived but also is
confirmed by the relatively high rse$4.822$, which could be

The predicted mark for Alice is $46.73$. taking into consideration rse
than we can assume that actual alice's mark will fall within bounds of
4.8 from predicted mark. Also here it would be more helpful to construct
prediction interval to assure Alice that she needs to study 8 hours to
maybe get dream mark.

to be sure that my point is right I will test on non linear regression

```{r}
model2 <- lm(Marks ~ time_study + I(time_study^2), data = data)
summary(model2)
```

and it is much better and it confirmed point above!
